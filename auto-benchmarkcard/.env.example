# LLM Configuration
# Required for RITS (Research IT Services) LLM backend
RITS_API_KEY=your_api_key_here
RITS_MODEL=meta-llama/llama-3-3-70b-instruct
RITS_API_URL=https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com
RITS_API_URL_RISK_ATLAS=https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com

# Optional: Ollama Configuration
# Uncomment if using local Ollama instead of RITS
# OLLAMA_API_URL=http://localhost:11434
# OLLAMA_MODEL=gemma3:27b

# Optional: Embedding Model
# Uncomment to override default
# EMBEDDING_MODEL=bge-large

# Optional: FactReasoner Configuration
# FACTREASONER_CACHE_DIR=factreasoner_cache
# MERLIN_PATH=external/merlin/bin/merlin
