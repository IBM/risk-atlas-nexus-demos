{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Metadata Extraction & Fact Verification Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow for extracting benchmark metadata, identifying risks, and performing fact-based verification using RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **UnitXT Lookup** - Extract benchmark metadata from catalog\n",
    "2. **ID Extraction** - Parse HuggingFace repo IDs and paper URLs\n",
    "3. **HuggingFace Metadata** - Fetch dataset information\n",
    "4. **Paper Extraction** - Extract content from academic papers using Docling\n",
    "5. **Card Composition** - Generate structured benchmark cards using LLM\n",
    "6. **Risk Identification** - Identify potential risks using Risk Atlas Nexus\n",
    "7. **RAG Processing** - Retrieve evidence for fact verification\n",
    "8. **Factuality Evaluation** - Assess confidence and flag uncertain fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "Install dependencies. FactReasoner and Risk Atlas Nexus are now installed from pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install package in development mode (run once)\n",
    "# !pip install -e ..\n",
    "\n",
    "# Dependencies are automatically installed:\n",
    "# - fact_reasoner @ git+https://github.com/arishofmann/FactReasoner.git\n",
    "# - risk-atlas-nexus[rits] @ git+https://github.com/IBM/risk-atlas-nexus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import workflow components\n",
    "from auto_benchmarkcard.workflow import build_workflow, OutputManager, sanitize_benchmark_name\n",
    "from auto_benchmarkcard.config import Config\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your API credentials and processing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment configured\n",
      "\\nLLM Engine: rits\n",
      "Model: llama-3.3-70b-instruct\n",
      "Threshold: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify credentials\n",
    "required = ['RITS_API_KEY', 'RITS_MODEL', 'RITS_API_URL']\n",
    "missing = [v for v in required if not os.getenv(v)]\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"Environment configured\")\n",
    "\n",
    "# Show config\n",
    "print(f\"LLM Engine: {Config.LLM_ENGINE_TYPE}\")\n",
    "print(f\"Model: {Config.DEFAULT_MODEL}\")\n",
    "print(f\"Threshold: {Config.DEFAULT_FACTUALITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Workflow\n",
    "\n",
    "Execute the complete pipeline for a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing benchmark: attaq\n"
     ]
    }
   ],
   "source": [
    "# Define benchmark to process\n",
    "BENCHMARK_QUERY = \"attaq\"  # Change this to your desired benchmark\n",
    "CATALOG_PATH = None  # Optional: custom UnitXT catalog path\n",
    "OUTPUT_PATH = None  # Optional: custom output directory\n",
    "\n",
    "print(f\"Processing benchmark: {BENCHMARK_QUERY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 10:45:10,316 - INFO - Looking up benchmark 'attaq' in UnitXT catalog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session: output/attaq_2025-10-16_10-45\n",
      "\\n=== Starting Workflow ===\\n\n",
      "'input_fields' field of Task should be a dictionary of field names and their types. For example, {'text': str, 'classes': List[str]}. Instead only '['input']' was passed. All types will be assumed to be 'Any'. In future version of unitxt this will raise an exception.\n",
      "For more information: see https://www.unitxt.ai/en/latest//docs/adding_task.html \n",
      "\n",
      "'reference_fields' field of Task should be a dictionary of field names and their types. For example, {'text': str, 'classes': List[str]}. Instead only '['label']' was passed. All types will be assumed to be 'Any'. In future version of unitxt this will raise an exception.\n",
      "For more information: see https://www.unitxt.ai/en/latest//docs/adding_task.html \n",
      "\n",
      "'prediction_type' was not set in Task. It is used to check the output of template post processors is compatible with the expected input of the metrics. Setting `prediction_type` to 'Any' (no checking is done). In future version of unitxt this will raise an exception.\n",
      "For more information: see https://www.unitxt.ai/en/latest//docs/adding_task.html \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 10:45:13,623 - INFO - Successfully retrieved UnitXT metadata for 'attaq' with 1 components\n",
      "2025-10-16 10:45:13,699 - INFO - UnitXT metadata retrieved\n",
      "2025-10-16 10:45:13,699 - INFO - Found: attaq - TaskCard delineates the phases in transforming the source da...\n",
      "2025-10-16 10:45:13,700 - INFO - UnitXT output saved to: output/attaq_2025-10-16_10-45/tool_output/unitxt/attaq.json\n",
      "2025-10-16 10:45:13,702 - INFO - Starting ID and URL extraction\n",
      "2025-10-16 10:45:13,703 - INFO - ID extraction completed\n",
      "2025-10-16 10:45:13,703 - INFO - Extracted: HF=ibm/AttaQ, Paper=None\n",
      "2025-10-16 10:45:13,704 - INFO - Extractor output saved to: output/attaq_2025-10-16_10-45/tool_output/extractor/attaq.json\n",
      "2025-10-16 10:45:13,705 - INFO - Fetching HuggingFace metadata for dataset: ibm/AttaQ\n",
      "2025-10-16 10:45:14,734 - INFO - Successfully retrieved HuggingFace metadata for ibm/AttaQ\n",
      "2025-10-16 10:45:14,734 - INFO - HuggingFace metadata retrieved successfully\n",
      "2025-10-16 10:45:14,735 - INFO - HuggingFace output saved to: output/attaq_2025-10-16_10-45/tool_output/hf/attaq.json\n",
      "2025-10-16 10:45:14,736 - INFO - Starting HuggingFace extraction\n",
      "2025-10-16 10:45:14,736 - INFO - No paper_url found in HF metadata\n",
      "2025-10-16 10:45:14,737 - INFO - Starting benchmark card composition\n",
      "2025-10-16 10:45:14,737 - INFO - Composing benchmark card for: attaq\n",
      "2025-10-16 10:45:14,738 - INFO - Available data sources: UnitXT, HuggingFace, Extracted IDs\n",
      "2025-10-16 10:45:14,738 - INFO - Generating Benchmark Details\n",
      "2025-10-16 10:45:23,781 - INFO - Benchmark Details completed\n",
      "2025-10-16 10:45:23,782 - INFO - Generating Purpose And Intended Users\n",
      "2025-10-16 10:45:32,322 - INFO - Purpose And Intended Users completed\n",
      "2025-10-16 10:45:32,323 - INFO - Generating Data\n",
      "2025-10-16 10:45:38,909 - INFO - Data completed\n",
      "2025-10-16 10:45:38,910 - INFO - Generating Methodology\n",
      "2025-10-16 10:45:45,162 - INFO - Methodology completed\n",
      "2025-10-16 10:45:45,162 - INFO - Generating Ethical And Legal Considerations\n",
      "2025-10-16 10:45:48,254 - INFO - Ethical And Legal Considerations completed\n",
      "2025-10-16 10:45:48,255 - INFO - Combining all sections into final benchmark card\n",
      "2025-10-16 10:45:48,255 - INFO - Final benchmark card assembled successfully\n",
      "2025-10-16 10:45:48,256 - INFO - Successfully composed benchmark card\n",
      "2025-10-16 10:45:48,256 - INFO - Card: AttaQ | text-generation, text2text-generation | English\n",
      "2025-10-16 10:45:48,257 - INFO - Starting risk identification\n",
      "2025-10-16 10:45:48,257 - INFO - ðŸ”„ Running Risk Atlas Nexus analysis...\n",
      "2025-10-16 10:45:48,257 - WARNING - Failed to create RITS inference engine: Invalid parameters found: ['max_tokens']. RITS inference engine only supports ['frequency_penalty', 'presence_penalty', 'max_completion_tokens', 'seed', 'stop', 'temperature', 'top_p', 'top_logprobs', 'logit_bias', 'logprobs', 'n', 'parallel_tool_calls', 'service_tier']\n",
      "2025-10-16 10:45:48,260 - WARNING - Risk identification will be skipped. Set RITS_API_KEY and RITS_API_URL_RISK_ATLAS environment variables.\n",
      "2025-10-16 10:45:48,260 - WARNING - No inference engine available - skipping risk identification\n",
      "2025-10-16 10:45:48,260 - WARNING - Risk identification failed - returning original benchmark card\n",
      "2025-10-16 10:45:48,261 - INFO - Risk-enhanced card saved to: output/attaq_2025-10-16_10-45/tool_output/risk_enhanced/attaq.json\n",
      "2025-10-16 10:45:48,261 - INFO - Risk identification completed\n",
      "2025-10-16 10:45:48,262 - INFO - Risks: None detected\n",
      "2025-10-16 10:45:48,262 - INFO - Starting RAG processing\n",
      "[2025-10-16 10:45:58:374] - INFO - RiskAtlasNexus - Created RITS inference engine.\n",
      "2025-10-16 10:46:20,142 - INFO - ðŸš€ Processing 20 statements with parallel reranking\n",
      "2025-10-16 10:46:35,286 - INFO - Starting parallel LLM reranking for 20 statements\n",
      "2025-10-16 10:47:26,695 - INFO - Parallel batch processing complete: 53 total chunks retrieved\n",
      "2025-10-16 10:47:26,697 - INFO - RAG processing completed\n",
      "2025-10-16 10:47:26,698 - INFO - RAG: 20 claims, 53 evidence sources\n",
      "2025-10-16 10:47:26,698 - INFO - RAG results saved to: output/attaq_2025-10-16_10-45/tool_output/rag/formatted_rag_results_attaq.jsonl\n",
      "2025-10-16 10:47:26,700 - INFO - Starting factuality evaluation\n",
      "2025-10-16 10:47:33,509 - INFO - âœ… FactReasoner evaluation complete\n",
      "2025-10-16 10:47:33,510 - INFO -    Factuality: 20 claims evaluated, 0/20 fields flagged\n",
      "2025-10-16 10:47:33,512 - INFO - FactReasoner evaluation complete\n",
      "2025-10-16 10:47:33,513 - INFO - Factuality: 20 claims evaluated, 20/20 fields flagged\n",
      "2025-10-16 10:47:33,513 - INFO - Factuality results saved to: output/attaq_2025-10-16_10-45/tool_output/factreasoner/factuality_results_attaq.json\n",
      "2025-10-16 10:47:33,513 - INFO - Final benchmark card saved to: output/attaq_2025-10-16_10-45/benchmarkcard/benchmark_card_attaq.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nâœ“ Workflow completed successfully!\\n\n"
     ]
    }
   ],
   "source": [
    "# Initialize output manager\n",
    "output_manager = OutputManager(BENCHMARK_QUERY, OUTPUT_PATH)\n",
    "print(f\"Session: {output_manager.get_summary()['session_directory']}\")\n",
    "\n",
    "# Initialize workflow state (all keys required by workflow)\n",
    "initial_state = {\n",
    "    \"query\": BENCHMARK_QUERY,\n",
    "    \"catalog_path\": CATALOG_PATH,\n",
    "    \"output_manager\": output_manager,\n",
    "    \"unitxt_json\": None,\n",
    "    \"extracted_ids\": None,\n",
    "    \"hf_repo\": None,\n",
    "    \"hf_json\": None,\n",
    "    \"docling_output\": None,\n",
    "    \"composed_card\": None,\n",
    "    \"risk_enhanced_card\": None,\n",
    "    \"completed\": [],\n",
    "    \"errors\": [],\n",
    "    \"hf_extraction_attempted\": False,\n",
    "    \"rag_results\": None,\n",
    "    \"factuality_results\": None,\n",
    "}\n",
    "\n",
    "# Build and execute workflow\n",
    "print(\"=== Starting Workflow ===\")\n",
    "workflow = build_workflow()\n",
    "\n",
    "try:\n",
    "    final_state = workflow.invoke(initial_state)\n",
    "    print(\"Workflow completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Workflow failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Workflow Steps Completed ===\\n\n",
      "1. unitxt done\n",
      "2. extract hf_repo=ibm/AttaQ, paper_url=None\n",
      "3. hf done\n",
      "4. hf_extract no paper_url found\n",
      "5. composer done\n",
      "6. risk identification done\n",
      "7. rag done\n",
      "8. factreasoner done\n"
     ]
    }
   ],
   "source": [
    "# Display workflow steps\n",
    "print(\"=== Workflow Steps Completed ===\")\n",
    "for i, step in enumerate(final_state.get('completed', []), 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "# Display errors if any\n",
    "errors = final_state.get('errors', [])\n",
    "if errors:\n",
    "    print(\"=== Errors ===\")\n",
    "    for error in errors:\n",
    "        print(f\"{error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View Benchmark Card\n",
    "\n",
    "Display the composed benchmark card with risk information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Details ===\\n\n",
      "Name: AttaQ\n",
      "Domains: ['text-generation', 'text2text-generation', 'safety', 'harm']\n",
      "Languages: ['English']\n",
      "\\nRisks Identified: 0\n",
      "\\n=== Full Benchmark Card ===\\n\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "benchmark_details": {
        "data_type": "text",
        "domains": [
         "text-generation",
         "text2text-generation",
         "safety",
         "harm"
        ],
        "languages": [
         "English"
        ],
        "name": "AttaQ",
        "overview": "The AttaQ red teaming dataset, consisting of 1402 carefully crafted adversarial questions, is designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses.",
        "resources": [
         "https://arxiv.org/abs/2311.04124",
         "https://huggingface.co/datasets/ibm/AttaQ"
        ],
        "similar_benchmarks": [
         "Not specified"
        ]
       },
       "data": {
        "annotation": "Semi-automatically labeled into seven categories: deception, discrimination, harmful information, substance abuse, sexual content, personally identifiable information (PII), and violence",
        "format": "CSV format with fields including label, input, and additional_info",
        "size": "1402 examples",
        "source": "Extracted from the hh-rlhf dataset provided by Anthropic, and generated using Large Language Models (LLMs) and Wikipedia Crimes Page"
       },
       "ethical_and_legal_considerations": {
        "compliance_with_regulations": "Not specified",
        "consent_procedures": "Not specified",
        "data_licensing": "MIT license",
        "privacy_and_anonymity": "Not specified"
       },
       "methodology": {
        "baseline_results": "Not specified",
        "calculation": "Harmlessness metric considers both the input and output, as described in the paper Unveiling Safety Vulnerabilities of Large Language Models",
        "interpretation": "Not specified",
        "methods": [
         "fine-tuning on task-specific data",
         "evaluation using a harmlessness metric"
        ],
        "metrics": [
         "harmlessness metric"
        ],
        "validation": "Evaluation using a metric that considers both the input and output, as detailed in the paper"
       },
       "purpose_and_intended_users": {
        "audience": [
         "Researchers",
         "Developers"
        ],
        "goal": "To evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, and to serve as a benchmark to assess the potential harm of responses produced by LLMs.",
        "limitations": "Limited to English language, focuses on text-generation and text2text-generation tasks, may not capture all aspects of language understanding, and contains offensive and upsetting content.",
        "out_of_scope_uses": [
         "Using the dataset to make models more harmful",
         "Real-time production systems without proper validation"
        ],
        "tasks": [
         "Evaluating LLMs for harmful responses"
        ]
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the risk-enhanced card\n",
    "risk_card = final_state.get('risk_enhanced_card', {}).get('benchmark_card')\n",
    "\n",
    "if risk_card:\n",
    "    details = risk_card.get('benchmark_details', {})\n",
    "    risks = risk_card.get('possible_risks', [])\n",
    "\n",
    "    print(\"=== Benchmark Details ===\")\n",
    "    print(f\"Name: {details.get('name', 'N/A')}\")\n",
    "    print(f\"Domains: {details.get('domains', [])}\")\n",
    "    print(f\"Languages: {details.get('languages', [])}\")\n",
    "    print(f\"\\\\nRisks Identified: {len(risks)}\")\n",
    "\n",
    "    # Display full card\n",
    "    print(\"=== Full Benchmark Card ===\")\n",
    "    display(JSON(risk_card, expanded=False))\n",
    "else:\n",
    "    print(\"No benchmark card available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Factuality Analysis\n",
    "\n",
    "Review confidence scores and flagged fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Factuality Statistics ===\\n\n",
      "Total Claims: 20\n",
      "High Confidence (â‰¥0.8): 0\n",
      "Low Confidence (<0.8): 20\n",
      "No Evidence: 20\n",
      "\\nâœ“ All fields verified!\n"
     ]
    }
   ],
   "source": [
    "factuality_results = final_state.get('factuality_results')\n",
    "\n",
    "if factuality_results:\n",
    "    marginals = factuality_results.get('marginals', [])\n",
    "    field_analysis = factuality_results.get('field_analysis', {})\n",
    "\n",
    "    # Count confidence levels\n",
    "    stats = {\n",
    "        'total': len(marginals),\n",
    "        'high': sum(1 for m in marginals if m.get('p_true', 0) >= 0.8),\n",
    "        'low': sum(1 for m in marginals if 0 < m.get('p_true', 0) < 0.8),\n",
    "        'none': sum(1 for m in marginals if m.get('p_true', 0) == 0.5)\n",
    "    }\n",
    "\n",
    "    print(\"=== Factuality Statistics ===\")\n",
    "    print(f\"Total Claims: {stats['total']}\")\n",
    "    print(f\"High Confidence (â‰¥0.8): {stats['high']}\")\n",
    "    print(f\"Low Confidence (<0.8): {stats['low']}\")\n",
    "    print(f\"No Evidence: {stats['none']}\")\n",
    "\n",
    "    # Show flagged fields\n",
    "    flagged = field_analysis.get('flagged_fields', [])\n",
    "    if flagged:\n",
    "        print(f\"\\\\nâš  Flagged Fields: {len(flagged)}\")\n",
    "        for field in flagged[:10]:\n",
    "            print(f\"  â€¢ {field}\")\n",
    "    else:\n",
    "        print(\"All fields verified!\")\n",
    "else:\n",
    "    print(\"No factuality results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Access Outputs\n",
    "\n",
    "All results are saved in the session directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Output Files ===\\n\n",
      "Session Directory: output/attaq_2025-10-16_10-45\n",
      "Benchmark Cards: output/attaq_2025-10-16_10-45/benchmarkcard\n",
      "\\nâœ“ Final card: output/attaq_2025-10-16_10-45/benchmarkcard/benchmark_card_attaq.json\n",
      "  Size: 5,438 bytes\n"
     ]
    }
   ],
   "source": [
    "# Display output locations\n",
    "summary = output_manager.get_summary()\n",
    "print(\"=== Output Files ===\\\\n\")\n",
    "print(f\"Session Directory: {summary['session_directory']}\")\n",
    "print(f\"Benchmark Cards: {summary['benchmark_cards']}\")\n",
    "\n",
    "# Final card path\n",
    "card_name = f\"benchmark_card_{sanitize_benchmark_name(BENCHMARK_QUERY)}.json\"\n",
    "card_path = Path(summary['benchmark_cards']) / card_name\n",
    "\n",
    "if card_path.exists():\n",
    "    print(f\"Final card: {card_path}\")\n",
    "    print(f\"  Size: {card_path.stat().st_size:,} bytes\")\n",
    "else:\n",
    "    print(f\"Card not found at: {card_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
