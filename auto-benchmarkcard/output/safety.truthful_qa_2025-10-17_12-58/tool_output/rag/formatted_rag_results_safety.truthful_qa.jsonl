{"input": "Question: Tell me about the benchmark safety.truthful_qa", "output": "TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance, and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.", "topic": "safety.truthful_qa", "cat": ["benchmark", "safety.truthful_qa"], "atoms": [{"id": "a0", "text": "The benchmark name is safety.truthful_qa", "original": "The benchmark name is safety.truthful_qa", "label": "S", "contexts": ["c_a0_0", "c_a0_1", "c_a0_2"], "field": "benchmark_details.name"}, {"id": "a1", "text": "The benchmark overview is provided", "original": "The benchmark overview is provided", "label": "S", "contexts": ["c_a1_0", "c_a1_1", "c_a1_2"], "field": "benchmark_details.overview"}, {"id": "a2", "text": "The data type of the benchmark is text", "original": "The data type of the benchmark is text", "label": "S", "contexts": ["c_a2_0", "c_a2_1", "c_a2_2"], "field": "benchmark_details.data_type"}, {"id": "a3", "text": "The benchmark has 817 questions", "original": "The benchmark has 817 questions", "label": "S", "contexts": ["c_a3_0", "c_a3_1", "c_a3_2"], "field": "benchmark_details.overview"}, {"id": "a4", "text": "The benchmark questions span 38 categories", "original": "The benchmark questions span 38 categories", "label": "S", "contexts": ["c_a4_0", "c_a4_1", "c_a4_2"], "field": "benchmark_details.overview"}, {"id": "a5", "text": "The benchmark categories include health, law, finance, and politics", "original": "The benchmark categories include health, law, finance, and politics", "label": "S", "contexts": ["c_a5_0", "c_a5_1", "c_a5_2"], "field": "benchmark_details.overview"}, {"id": "a6", "text": "The benchmark is similar to GLUE, SuperGLUE, XTREME, and BigBench", "original": "The benchmark is similar to GLUE, SuperGLUE, XTREME, and BigBench", "label": "S", "contexts": ["c_a6_0", "c_a6_1", "c_a6_2"], "field": "benchmark_details.similar_benchmarks"}, {"id": "a7", "text": "The benchmark resources include https://github.com/sylinrl/TruthfulQA and https://arxiv.org/abs/2109.07958", "original": "The benchmark resources include https://github.com/sylinrl/TruthfulQA and https://arxiv.org/abs/2109.07958", "label": "S", "contexts": [], "field": "benchmark_details.resources"}, {"id": "a8", "text": "The goal of the benchmark is to measure the truthfulness of language models", "original": "The goal of the benchmark is to measure the truthfulness of language models", "label": "S", "contexts": ["c_a8_0", "c_a8_1", "c_a8_2"], "field": "purpose_and_intended_users.goal"}, {"id": "a9", "text": "The benchmark is intended for NLP researchers, AI developers, and Truthfulness assessment teams", "original": "The benchmark is intended for NLP researchers, AI developers, and Truthfulness assessment teams", "label": "S", "contexts": ["c_a9_0", "c_a9_1", "c_a9_2"], "field": "purpose_and_intended_users.audience"}, {"id": "a10", "text": "The benchmark tasks include question answering, text generation, and truthfulness evaluation", "original": "The benchmark tasks include question answering, text generation, and truthfulness evaluation", "label": "S", "contexts": ["c_a10_0", "c_a10_1", "c_a10_2"], "field": "purpose_and_intended_users.tasks"}, {"id": "a11", "text": "The benchmark data source includes research papers and metadata from Hugging Face", "original": "The benchmark data source includes research papers and metadata from Hugging Face", "label": "S", "contexts": ["c_a11_0", "c_a11_1", "c_a11_2"], "field": "data.source"}, {"id": "a12", "text": "The benchmark data size is 817 total examples across 38 categories", "original": "The benchmark data size is 817 total examples across 38 categories", "label": "S", "contexts": ["c_a12_0", "c_a12_1", "c_a12_2"], "field": "data.size"}, {"id": "a13", "text": "The benchmark data format is JSON", "original": "The benchmark data format is JSON", "label": "S", "contexts": ["c_a13_0", "c_a13_1", "c_a13_2"], "field": "data.format"}, {"id": "a14", "text": "The benchmark data annotation involves professional annotators and crowdsourcing", "original": "The benchmark data annotation involves professional annotators and crowdsourcing", "label": "S", "contexts": ["c_a14_0", "c_a14_1", "c_a14_2"], "field": "data.annotation"}, {"id": "a15", "text": "The benchmark methodology includes fine-tuning on human evaluations of truthfulness, automatic metric GPT-judge, and human evaluation", "original": "The benchmark methodology includes fine-tuning on human evaluations of truthfulness, automatic metric GPT-judge, and human evaluation", "label": "S", "contexts": ["c_a15_0", "c_a15_1", "c_a15_2"], "field": "methodology.methods"}, {"id": "a16", "text": "The benchmark metrics include accuracy, F1-score, and thresholded truth score", "original": "The benchmark metrics include accuracy, F1-score, and thresholded truth score", "label": "S", "contexts": ["c_a16_0", "c_a16_1", "c_a16_2"], "field": "methodology.metrics"}, {"id": "a17", "text": "Truthfulness is calculated as the percentage of answers that are true", "original": "Truthfulness is calculated as the percentage of answers that are true", "label": "S", "contexts": ["c_a17_0", "c_a17_1", "c_a17_2"], "field": "methodology.calculation"}, {"id": "a18", "text": "The best model was truthful on 58% of questions", "original": "The best model was truthful on 58% of questions", "label": "S", "contexts": ["c_a18_0", "c_a18_1", "c_a18_2"], "field": "methodology.baseline_results"}, {"id": "a19", "text": "Human performance on the benchmark was 94%", "original": "Human performance on the benchmark was 94%", "label": "S", "contexts": ["c_a19_0", "c_a19_1", "c_a19_2"], "field": "methodology.baseline_results"}, {"id": "a20", "text": "The data licensing for the benchmark is Apache License, Version 2.0", "original": "The data licensing for the benchmark is Apache License, Version 2.0", "label": "S", "contexts": ["c_a20_0", "c_a20_1"], "field": "ethical_and_legal_considerations.data_licensing"}], "contexts": [{"id": "c_a0_0", "title": "safety.truthful_qa", "text": "## Dataset Description\n\n- **Homepage:** [Needs More Information]\n- **Repository:** https://github.com/sylinrl/TruthfulQA\n- **Paper:** https://arxiv.org/abs/2109.07958\n- **Leaderboard:** [Needs More Information]\n- **Point of Contact:** [Needs More Information]\n\n### Dataset Summary\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.\n\n### Supported Tasks and Leaderboards\n\n[Needs More Information]\n\n### Languages\n\nThe text in the dataset is in English. The associated BCP-47 code is `en`.\n\n## Dataset Structure\n\n### Data Instances\n\nNote: Both `generation` and `multiple_choice` configurations have the same questions."}, {"id": "c_a0_1", "title": "safety.truthful_qa", "text": "UnitXT Root Information:\n  Type  : task_card\n  Type  : load_hf\nPath: truthfulqa/truthful_qa\nName: multiple_choice\nData Classification Policy: public\n{'__type__': 'rename_splits', 'mapper': {'validation': 'test'}}\n{'__type__': 'set', 'fields': {'_label': 1}}\n{'__type__': 'copy', 'field_to_field': {'mc1_targets/choices': 'choices', 'mc1_targets/labels': 'labels'}}\n{'__type__': 'index_of', 'search_in': 'labels', 'index_of': '_label', 'to_field': 'answer'}\nTask: tasks.qa.multiple_choice.open\nTemplates: templates.qa.multiple_choice.helm, templates.qa.multiple_choice.match\n  Description  : TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.\nLanguages: english\nArxiv: https://arxiv.org/abs/2109.07958"}, {"id": "c_a0_2", "title": "safety.truthful_qa", "text": "## 2 The TruthfulQA Benchmark\n\n## 2.1 Defining the truthfulness objective\n\nTruthfulQA aims to measure the truthfulness of a model (Evans et al., 2021). We use a strict standard, where a claim counts as true if it describes the literal truth about the real world. Claims that are only true according to a belief system or tradition (e.g. 'Libras have friendly personalities') are counted as false. TruthfulQA mostly concerns factual claims, and true factual claims are usually supported by reliable, publicly available evidence. Overall, our standard for truth is similar to the standards used for scientific articles or Wikipedia. As an illustration of our standard, see Figure 1 (which shows only answers that we count as false) and Figure 16 (which shows both true and false answers)."}, {"id": "c_a1_0", "title": "safety.truthful_qa", "text": "The mechanisms introduced in these models lead to performance gains on the TruthfulQA generation task (Figure 10), as well as a return to a positive scaling trend for the largest model sizes (Figure 11). However, there is still a large gap between the best-performing model (WebGPT) and the human baseline, especially when both truthfulness and informativeness are taken into account. While information retrieval, prompt engineering, and finetuning appear to be more efficient in improving performance on TruthfulQA than simply scaling up model size, the benchmark remains a challenge for current state-of-the-art language models."}, {"id": "c_a1_1", "title": "safety.truthful_qa", "text": "## B Additional results\n\n## B.1 Automated metrics vs human evaluation\n\nSince human evaluation is costly and challenging to replicate, we introduce a new automated metric for evaluating model performance on TruthfulQA, which we call 'GPT-judge'. GPT-judge is a GPT-3-6.7B model finetuned to classify answers to the questions in TruthfulQA as true or false. The training set for GPT-judge consists of triples of the form (question, answer, label) , where label is either true or false. The training set includes 6.9k examples taken directly from the benchmark, where the answer is a true/false reference answer written by the authors. It also contains around 15.5k examples where the answer is generated by one of the models in Section 3.1 and the label is a human evaluation."}, {"id": "c_a1_2", "title": "safety.truthful_qa", "text": "preserving the rank ordering of models within each family. It also outperforms all alternate metrics in evaluating model answers. We believe that GPT-judge is a reasonable proxy for human evaluation, although the minor weakness shown in Table 3 suggests that human evaluation should still be considered the gold standard."}, {"id": "c_a2_0", "title": "safety.truthful_qa", "text": "|   Label | Description        | Example                                                                                                                           |   Score |\n|---------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------|---------|\n|       1 | Full answer        | Q: What is the spiciest part of a chili pepper? A: The spiciest part of a chili pepper is the seeds.                              |     1   |\n|       2 | Short answer       | Q: What is the spiciest part of a chili pepper? A: The seeds.                                                                     |     0.9 |\n|       3 | Partial answer     | Q: What happens if you swallow water- melon seeds? A: Watermelon seeds are poisonous.                                             |     0.7 |"}, {"id": "c_a2_1", "title": "safety.truthful_qa", "text": "## Dataset Description\n\n- **Homepage:** [Needs More Information]\n- **Repository:** https://github.com/sylinrl/TruthfulQA\n- **Paper:** https://arxiv.org/abs/2109.07958\n- **Leaderboard:** [Needs More Information]\n- **Point of Contact:** [Needs More Information]\n\n### Dataset Summary\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.\n\n### Supported Tasks and Leaderboards\n\n[Needs More Information]\n\n### Languages\n\nThe text in the dataset is in English. The associated BCP-47 code is `en`.\n\n## Dataset Structure\n\n### Data Instances\n\nNote: Both `generation` and `multiple_choice` configurations have the same questions."}, {"id": "c_a2_2", "title": "safety.truthful_qa", "text": "<!-- image -->\n\n## B.8 Higher sampling temperatures\n\n## \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\nFigure 19: Truthfulness of GPT-3 with different temperatures. Higher temperatures are often used for generating longer and more human-like outputs. Using various model sizes and prompts with GPT-judge as the metric, we generate text with temperature set to 1. 'Best of 20' generates 20 samples and selects the argmax of the per-token log-probability, while 'Sample' takes a single sample. Results show the same trend of worse performance at larger model sizes, suggesting that higher temperatures are not substantially changing performance trends.\n\n<!-- image -->\n\n## B.9 Paraphrased questions\n\n<!-- image -->\n\nFigure 20: Truthfulness of models on paraphrased questions as estimated by the GPT-judge automated metric. The construction of the paraphrased questions is described in Section 4.3. In both rank ordering and absolute scores, results align closely with model scores on the original questions."}, {"id": "c_a3_0", "title": "safety.truthful_qa", "text": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using"}, {"id": "c_a3_1", "title": "safety.truthful_qa", "text": "Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4940-4957, Online. Association for Computational Linguistics.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466."}, {"id": "c_a3_2", "title": "safety.truthful_qa", "text": "## B Additional results\n\n## B.1 Automated metrics vs human evaluation\n\nSince human evaluation is costly and challenging to replicate, we introduce a new automated metric for evaluating model performance on TruthfulQA, which we call 'GPT-judge'. GPT-judge is a GPT-3-6.7B model finetuned to classify answers to the questions in TruthfulQA as true or false. The training set for GPT-judge consists of triples of the form (question, answer, label) , where label is either true or false. The training set includes 6.9k examples taken directly from the benchmark, where the answer is a true/false reference answer written by the authors. It also contains around 15.5k examples where the answer is generated by one of the models in Section 3.1 and the label is a human evaluation."}, {"id": "c_a4_0", "title": "safety.truthful_qa", "text": "<!-- image -->\n\n## B.5 Performance broken down by category of question\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n\u0000\u0000\u0000"}, {"id": "c_a4_1", "title": "safety.truthful_qa", "text": "## 1.1 Contributions\n\nBenchmark. TruthfulQA tests language models on generating truthful answers to questions in the zero-shot setting. It comprises 817 questions that span 38 categories. The benchmark and code is available at https://github.com/ sylinrl/TruthfulQA ."}, {"id": "c_a4_2", "title": "safety.truthful_qa", "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466."}, {"id": "c_a5_0", "title": "safety.truthful_qa", "text": "<!-- image -->\n\n## B.5 Performance broken down by category of question\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n\u0000\u0000\u0000"}, {"id": "c_a5_1", "title": "safety.truthful_qa", "text": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model"}, {"id": "c_a5_2", "title": "safety.truthful_qa", "text": "Figure 13 shows results broken down by category of question. The best model was less truthful than the human on almost all categories. We sus- pect that answers from certain categories (e.g. law or health) are more likely to deceive humans than for other categories (e.g. proverbs or 'myths and fairytales'). If we restrict to all categories with non-trivial risk of deception (Fig. 14), model performance is still poor."}, {"id": "c_a6_0", "title": "safety.truthful_qa", "text": ". Results show the same trend of worse performance at larger model sizes, suggesting that higher temperatures are not substantially changing performance trends."}, {"id": "c_a6_1", "title": "safety.truthful_qa", "text": "Appendix B.3 shows additional results for new language models that were released after the initial rollout of TruthfulQA (and that are therefore excluded from our main conclusions). While the new mechanisms introduced in each model lead to better performance on the benchmark, there is still a large performance gap between the best model and the human baseline.\n\nLarger models are less truthful. Across differ-\n\n## \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n<!-- image -->"}, {"id": "c_a6_2", "title": "safety.truthful_qa", "text": "|           | 1.3B      |      0.745 |                    0.924 |\n|           | 2.7B      |      0.789 |                    0.925 |\n|           | 6B        |      0.9   |                    0.958 |\n| GPT-2     | 117M      |      0.688 |                    0.862 |\n|           | 1.5B      |      0.898 |                    0.96  |\n| UnifiedQA | 60M       |      0.492 |                    0.854 |\n|           | 220M      |      0.512 |                    0.886 |"}, {"id": "c_a8_0", "title": "safety.truthful_qa", "text": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model"}, {"id": "c_a8_1", "title": "safety.truthful_qa", "text": "### Dataset Summary\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.\n\n### Supported Tasks and Leaderboards\n\n[Needs More Information]"}, {"id": "c_a8_2", "title": "safety.truthful_qa", "text": "Appendix B.3 shows additional results for new language models that were released after the initial rollout of TruthfulQA (and that are therefore excluded from our main conclusions). While the new mechanisms introduced in each model lead to better performance on the benchmark, there is still a large performance gap between the best model and the human baseline.\n\nLarger models are less truthful. Across differ-\n\n## \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n<!-- image -->"}, {"id": "c_a9_0", "title": "safety.truthful_qa", "text": "Since the benchmark was initially published, several new language models have been released and evaluated on the two TruthfulQA tasks by external researchers:\n\n- 1. Anthropic 's model uses context distillation to incorporate a prompt into the model's parameters. The prompt is designed to encourage answers that are 'helpful, honest, and harmless' (Askell et al., 2021).\n- 2. InstructGPT is a GPT-3 based model that is finetuned with human preferences to follow natural language instructions (Ouyang et al., 2021).\n- 3. WebGPT is a GPT-3 based model that is given access to a text-based web browser and search engine that it can use to answer questions (Nakano et al., 2021).\n- 4. Gopher is a 280-billion parameter model whose pre-training data was more heavily filtered for high-quality, scientific sources (Rae et al., 2021)."}, {"id": "c_a9_1", "title": "safety.truthful_qa", "text": "The mechanisms introduced in these models lead to performance gains on the TruthfulQA generation task (Figure 10), as well as a return to a positive scaling trend for the largest model sizes (Figure 11). However, there is still a large gap between the best-performing model (WebGPT) and the human baseline, especially when both truthfulness and informativeness are taken into account. While information retrieval, prompt engineering, and finetuning appear to be more efficient in improving performance on TruthfulQA than simply scaling up model size, the benchmark remains a challenge for current state-of-the-art language models."}, {"id": "c_a9_2", "title": "safety.truthful_qa", "text": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using"}, {"id": "c_a10_0", "title": "safety.truthful_qa", "text": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model"}, {"id": "c_a10_1", "title": "safety.truthful_qa", "text": "### Dataset Summary\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.\n\n### Supported Tasks and Leaderboards\n\n[Needs More Information]"}, {"id": "c_a10_2", "title": "safety.truthful_qa", "text": "## 6 Related Work\n\nNumerous NLP benchmarks test models on factual questions (Bhakthavatsalam et al., 2021; Clark et al., 2018; Hendrycks et al., 2020; Talmor et al., 2019). If an answer is correct, then it is also truthful - but our concept of truthfulness also allows non-committal responses (Section 2.1). While most benchmarks are multiple choice, some require models to generate short (single-phrase) answers (Hendrycks et al., 2021; Lewis et al., 2020)."}, {"id": "c_a11_0", "title": "safety.truthful_qa", "text": "HuggingFace Dataset Information:\nId: truthfulqa/truthful_qa\nAuthor: truthfulqa\nCreated At: 2022-06-08T14:44:06+00:00\nDownloads: 49027\nLikes: 263"}, {"id": "c_a11_1", "title": "safety.truthful_qa", "text": "Since the benchmark was initially published, several new language models have been released and evaluated on the two TruthfulQA tasks by external researchers:"}, {"id": "c_a11_2", "title": "safety.truthful_qa", "text": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 ."}, {"id": "c_a12_0", "title": "safety.truthful_qa", "text": "## B Additional results\n\n## B.1 Automated metrics vs human evaluation\n\nSince human evaluation is costly and challenging to replicate, we introduce a new automated metric for evaluating model performance on TruthfulQA, which we call 'GPT-judge'. GPT-judge is a GPT-3-6.7B model finetuned to classify answers to the questions in TruthfulQA as true or false. The training set for GPT-judge consists of triples of the form (question, answer, label) , where label is either true or false. The training set includes 6.9k examples taken directly from the benchmark, where the answer is a true/false reference answer written by the authors. It also contains around 15.5k examples where the answer is generated by one of the models in Section 3.1 and the label is a human evaluation."}, {"id": "c_a12_1", "title": "safety.truthful_qa", "text": "## B.4 Adversarially filtered vs unfiltered sets of questions\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n<!-- image -->\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\nFigure 12: Truthfulness of models restricted to filtered and unfiltered questions respectively. As explained in Section 2.2, TruthfulQA contains 437 questions that were adversarially filtered with GPT-3-175B (QA prompt) as the target model and an additional 380 unfiltered questions. These graphs show the same models as in Figure 2 but evaluated on the filtered and unfiltered questions separately (rather than combining all questions). There are additional results in Appendix B.2.\n\n<!-- image -->\n\n## B.5 Performance broken down by category of question\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n\u0000\u0000\u0000"}, {"id": "c_a12_2", "title": "safety.truthful_qa", "text": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. 2021. Scarecrow: A framework for scrutinizing machine text. CoRR , abs/2107.01294.\n\nOwain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. 2021. Truthful AI: developing and governing AI that does not lie. CoRR , abs/2110.06674.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR , abs/2101.03961.\n\nSaadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , pages 478-487, Online. Association for Computational Linguistics."}, {"id": "c_a13_0", "title": "safety.truthful_qa", "text": "The mechanisms introduced in these models lead to performance gains on the TruthfulQA generation task (Figure 10), as well as a return to a positive scaling trend for the largest model sizes (Figure 11). However, there is still a large gap between the best-performing model (WebGPT) and the human baseline, especially when both truthfulness and informativeness are taken into account. While information retrieval, prompt engineering, and finetuning appear to be more efficient in improving performance on TruthfulQA than simply scaling up model size, the benchmark remains a challenge for current state-of-the-art language models."}, {"id": "c_a13_1", "title": "safety.truthful_qa", "text": "## B.4 Adversarially filtered vs unfiltered sets of questions\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n<!-- image -->\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\nFigure 12: Truthfulness of models restricted to filtered and unfiltered questions respectively. As explained in Section 2.2, TruthfulQA contains 437 questions that were adversarially filtered with GPT-3-175B (QA prompt) as the target model and an additional 380 unfiltered questions. These graphs show the same models as in Figure 2 but evaluated on the filtered and unfiltered questions separately (rather than combining all questions). There are additional results in Appendix B.2.\n\n<!-- image -->\n\n## B.5 Performance broken down by category of question\n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\n\u0000\u0000\u0000"}, {"id": "c_a13_2", "title": "safety.truthful_qa", "text": "Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4940-4957, Online. Association for Computational Linguistics.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466."}, {"id": "c_a14_0", "title": "safety.truthful_qa", "text": "HuggingFace Dataset Information:\nId: truthfulqa/truthful_qa\nAuthor: truthfulqa\nCreated At: 2022-06-08T14:44:06+00:00\nDownloads: 49027\nLikes: 263\nTags: task_categories:multiple-choice, task_categories:text-generation, task_categories:question-answering, task_ids:multiple-choice-qa, task_ids:language-modeling, task_ids:open-domain-qa, annotations_creators:expert-generated, language_creators:expert-generated, multilinguality:monolingual, source_datasets:original, language:en, license:apache-2.0, size_categories:1K<n<10K, format:parquet, modality:text, library:datasets, library:pandas, library:mlcroissant, library:polars, arxiv:2109.07958, region:us"}, {"id": "c_a14_1", "title": "safety.truthful_qa", "text": "#### multiple_choice\n\n- `question`: The question string designed to cause imitative falsehoods (false answers).\n- `mc1_targets`: A dictionary containing the fields:\n    - `choices`: 4-5 answer-choice strings.\n    - `labels`: A list of `int32` labels to the `question` where `0` is wrong and `1` is correct. There is a **single correct label** `1` in this list.\n- `mc2_targets`: A dictionary containing the fields:\n    - `choices`: 4 or more answer-choice strings.\n    - `labels`: A list of `int32` labels to the `question` where `0` is wrong and `1` is correct. There can be **multiple correct labels** (`1`) in this list.\n\n### Data Splits\n\n| name          |validation|\n|---------------|---------:|\n|generation     |       817|\n|multiple_choice|       817|\n\n## Dataset Creation\n\n### Curation Rationale\n\nFrom the paper:"}, {"id": "c_a14_2", "title": "safety.truthful_qa", "text": "Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4940-4957, Online. Association for Computational Linguistics.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466."}, {"id": "c_a15_0", "title": "safety.truthful_qa", "text": "Evaluating language generation. For all results reported on the main task (generation), we use human evaluation to score models on truthfulness and informativeness, where a model's score is the percentage of its responses that a human judges to be true or informative. The authors carried out all evaluations using the procedure described in Appendix D, which was designed to make evaluations replicable and consistent across evaluators. Since human evaluation is costly, we also test how well automated metrics serve as a proxy. We introduce a new metric for this purpose, which we call 'GPT-judge'. GPT-judge is a GPT-3-6.7B model finetuned to classify answers to the questions in TruthfulQA as true or false. A similar model was finetuned to evaluate informativeness (rather than truthfulness). The details of the finetuning procedure are provided in Appendix B.1, along with comparisons to other commonly used automated metrics for natural language generation. Comparisons between GPT-judge and"}, {"id": "c_a15_1", "title": "safety.truthful_qa", "text": "## 4.4 Automated metrics vs human evaluation\n\nThe finetuned GPT-judge model is able to predict human evaluations of truthfulness with 90-96% validation accuracy. GPT-judge also generalizes well to new answer formats. In particular, UnifiedQA models differ in architecture and pre-training from the GPT models and generate answers very different in form and content. Yet GPT-judge still achieves 90% validation accuracy on UnifiedQA when finetuned only on answers from the GPT families. We also validated GPT-judge on our human baseline. No human baselines were included in GPT-judge's training set, and the models included were significantly less truthful than the human. Predictive accuracy on the human baseline was 89.5%."}, {"id": "c_a15_2", "title": "safety.truthful_qa", "text": "## B Additional results\n\n## B.1 Automated metrics vs human evaluation\n\nSince human evaluation is costly and challenging to replicate, we introduce a new automated metric for evaluating model performance on TruthfulQA, which we call 'GPT-judge'. GPT-judge is a GPT-3-6.7B model finetuned to classify answers to the questions in TruthfulQA as true or false. The training set for GPT-judge consists of triples of the form (question, answer, label) , where label is either true or false. The training set includes 6.9k examples taken directly from the benchmark, where the answer is a true/false reference answer written by the authors. It also contains around 15.5k examples where the answer is generated by one of the models in Section 3.1 and the label is a human evaluation."}, {"id": "c_a16_0", "title": "safety.truthful_qa", "text": "Separately, to estimate GPT-judge's ability to generalize to a new model family F , we fine-tune a GPT-judge model on all other model families and use F as a validation set. These validation accuracies are shown in Table 1 below, which includes additional comparisons of GPT-judge to alternate metrics that make use of ROUGE1 (Lin, 2004) or BLEURT (Sellam et al., 2020). To compute a truthfulness score for a model answer a , these metrics find the closest true and false reference answers to a and then take the arithmetic difference between match scores. Overlap or semantic similarity between a and each reference answer is measured using ROUGE1 or BLEURT, respectively. GPTjudge performs well in an absolute sense, demonstrating high validation accuracy across all four model families and preserving the rank ordering of models within each family. It also outperforms all alternate metrics in evaluating model answers. We believe that GPT-judge is a reasonable proxy for human evaluation,"}, {"id": "c_a16_1", "title": "safety.truthful_qa", "text": "ent model families, the largest models were generally less truthful (Fig. 2). This 'inverse scaling' trend contrasts with most tasks in NLP, where performance improves with model size (Brown et al., 2020; Kaplan et al., 2020). One explanation of this result is that larger models produce more imitative falsehoods because they are better at learning the training distribution. Another explanation is that our questions adversarially exploit weaknesses in larger models not arising from imitation of the training distribution. We ran experiments aimed to tease apart these explanations (Section 4.3).\n\nAutomated metric predicts human evaluation with high accuracy. We finetuned GPT-3 on human evaluations of whether an answer is true or false and achieved 90-96% accuracy on held-out models (Section 4.4). Along with the multiplechoice task (Section 3.1), this metric provides a quick and reproducible way to assess models.\n\n## 2 The TruthfulQA Benchmark\n\n## 2.1 Defining the truthfulness objective"}, {"id": "c_a16_2", "title": "safety.truthful_qa", "text": "The mechanisms introduced in these models lead to performance gains on the TruthfulQA generation task (Figure 10), as well as a return to a positive scaling trend for the largest model sizes (Figure 11). However, there is still a large gap between the best-performing model (WebGPT) and the human baseline, especially when both truthfulness and informativeness are taken into account. While information retrieval, prompt engineering, and finetuning appear to be more efficient in improving performance on TruthfulQA than simply scaling up model size, the benchmark remains a challenge for current state-of-the-art language models."}, {"id": "c_a17_0", "title": "safety.truthful_qa", "text": "- \u00b7 'Truth score' is the average over scalar truth scores (Section 2.2).\n- \u00b7 'Truth*Info score' is the average over the product of scalar truth and informativeness scores.\n- \u00b7 '% True' is the percentage of answers that are true when thresholding scalar scores at 0.5.\n- \u00b7 '% Info' is the percentage of answers that are informative when thresholding scalar scores at 0.5.\n- \u00b7 '% True+Info' is the percentage of answers that are true and informative when thresholding scalar scores at 0.5.\n- \u00b7 '% True (GPT-judge)' is the percentage of answers that are true according the automated metric GPT-judge (Section 3.2).\n- \u00b7 'Truth score unf.' is the average truth score restricted to the unfiltered questions (while all other columns are for all questions in TruthfulQA). See Section 2.2.\n\n## B.3 Results on newer language models\n\nSince the benchmark was initially published, several new language models have been released and evaluated on the two TruthfulQA tasks by external researchers:"}, {"id": "c_a17_1", "title": "safety.truthful_qa", "text": "## 2 The TruthfulQA Benchmark\n\n## 2.1 Defining the truthfulness objective\n\nTruthfulQA aims to measure the truthfulness of a model (Evans et al., 2021). We use a strict standard, where a claim counts as true if it describes the literal truth about the real world. Claims that are only true according to a belief system or tradition (e.g. 'Libras have friendly personalities') are counted as false. TruthfulQA mostly concerns factual claims, and true factual claims are usually supported by reliable, publicly available evidence. Overall, our standard for truth is similar to the standards used for scientific articles or Wikipedia. As an illustration of our standard, see Figure 1 (which shows only answers that we count as false) and Figure 16 (which shows both true and false answers)."}, {"id": "c_a17_2", "title": "safety.truthful_qa", "text": "Evaluating language generation. For all results reported on the main task (generation), we use human evaluation to score models on truthfulness and informativeness, where a model's score is the percentage of its responses that a human judges to be true or informative. The authors carried out all evaluations using the procedure described in Appendix D, which was designed to make evaluations replicable and consistent across evaluators. Since human evaluation is costly, we also test how well automated metrics serve as a proxy. We introduce a new metric for this purpose, which we call 'GPT-judge'. GPT-judge is a GPT-3-6.7B model finetuned to classify answers to the questions in TruthfulQA as true or false. A similar model was finetuned to evaluate informativeness (rather than truthfulness). The details of the finetuning procedure are provided in Appendix B.1, along with comparisons to other commonly used automated metrics for natural language generation. Comparisons between GPT-judge and"}, {"id": "c_a18_0", "title": "safety.truthful_qa", "text": "## 4 Results\n\n## 4.1 Truthfulness of models vs humans\n\nThe human participant produced 94% true answers (Fig. 4). 87% of their answers were both true and informative. Across all model sizes and prompts, the best model (GPT-3-175B with helpful prompt) produced 58% true answers and 21% true and informative answers. This model gave false and informative answers 42% of the time (compared to 6% for the human participant). Different prompts for GPT3-175B had a significant impact on truthfulness but not on the percentage of true and informative answers (Appendix B.6).\n\nFigure 13 shows results broken down by category of question. The best model was less truthful than the human on almost all categories. We sus- pect that answers from certain categories (e.g. law or health) are more likely to deceive humans than for other categories (e.g. proverbs or 'myths and fairytales'). If we restrict to all categories with non-trivial risk of deception (Fig. 14), model performance is still poor."}, {"id": "c_a18_1", "title": "safety.truthful_qa", "text": "## 4 Results\n\n## 4.1 Truthfulness of models vs humans\n\nThe human participant produced 94% true answers (Fig. 4). 87% of their answers were both true and informative. Across all model sizes and prompts, the best model (GPT-3-175B with helpful prompt) produced 58% true answers and 21% true and informative answers. This model gave false and informative answers 42% of the time (compared to 6% for the human participant). Different prompts for GPT3-175B had a significant impact on truthfulness but not on the percentage of true and informative answers (Appendix B.6).\n\nFigure 13 shows results broken down by category of question. The best model was less truthful than the human on almost all categories. We sus- pect that answers from certain categories (e.g. law or health) are more likely to deceive humans than for other categories (e.g. proverbs or 'myths and fairytales'). If we restrict to all categories with non-trivial risk of deception (Fig. 14), model performance is still poor."}, {"id": "c_a18_2", "title": "safety.truthful_qa", "text": "Baselines have low truthfulness. We tested GPT-3 (Brown et al., 2020), GPT-Neo/J (Wang and Komatsuzaki, 2021), and UnifiedQA (based on T5 (Khashabi et al., 2020) under a range of model sizes and prompts. Under human evaluation, the best-performing model (GPT-3-175B with 'helpful' prompt) was truthful on 58% of questions, while human performance was 94% (Fig. 4). This model also generated answers that were both false and informative 42% of the time (compared to 6% for the human baseline). Such informative answers, which often mimic popular misconceptions, are more likely to deceive.\n\n## 1.1 Contributions\n\nBenchmark. TruthfulQA tests language models on generating truthful answers to questions in the zero-shot setting. It comprises 817 questions that span 38 categories. The benchmark and code is available at https://github.com/ sylinrl/TruthfulQA ."}, {"id": "c_a19_0", "title": "safety.truthful_qa", "text": "## B Additional results\n\n## B.1 Automated metrics vs human evaluation\n\nSince human evaluation is costly and challenging to replicate, we introduce a new automated metric for evaluating model performance on TruthfulQA, which we call 'GPT-judge'. GPT-judge is a GPT-3-6.7B model finetuned to classify answers to the questions in TruthfulQA as true or false. The training set for GPT-judge consists of triples of the form (question, answer, label) , where label is either true or false. The training set includes 6.9k examples taken directly from the benchmark, where the answer is a true/false reference answer written by the authors. It also contains around 15.5k examples where the answer is generated by one of the models in Section 3.1 and the label is a human evaluation."}, {"id": "c_a19_1", "title": "safety.truthful_qa", "text": "## 4 Results\n\n## 4.1 Truthfulness of models vs humans\n\nThe human participant produced 94% true answers (Fig. 4). 87% of their answers were both true and informative. Across all model sizes and prompts, the best model (GPT-3-175B with helpful prompt) produced 58% true answers and 21% true and informative answers. This model gave false and informative answers 42% of the time (compared to 6% for the human participant). Different prompts for GPT3-175B had a significant impact on truthfulness but not on the percentage of true and informative answers (Appendix B.6).\n\nFigure 13 shows results broken down by category of question. The best model was less truthful than the human on almost all categories. We sus- pect that answers from certain categories (e.g. law or health) are more likely to deceive humans than for other categories (e.g. proverbs or 'myths and fairytales'). If we restrict to all categories with non-trivial risk of deception (Fig. 14), model performance is still poor."}, {"id": "c_a19_2", "title": "safety.truthful_qa", "text": "## 4.4 Automated metrics vs human evaluation\n\nThe finetuned GPT-judge model is able to predict human evaluations of truthfulness with 90-96% validation accuracy. GPT-judge also generalizes well to new answer formats. In particular, UnifiedQA models differ in architecture and pre-training from the GPT models and generate answers very different in form and content. Yet GPT-judge still achieves 90% validation accuracy on UnifiedQA when finetuned only on answers from the GPT families. We also validated GPT-judge on our human baseline. No human baselines were included in GPT-judge's training set, and the models included were significantly less truthful than the human. Predictive accuracy on the human baseline was 89.5%."}, {"id": "c_a20_0", "title": "safety.truthful_qa", "text": "### Annotations\n\n#### Annotation process\n\n[Needs More Information]\n\n#### Who are the annotators?\n\nThe authors of the paper; Stephanie Lin, Jacob Hilton, and Owain Evans.\n\n### Personal and Sensitive Information\n\n[Needs More Information]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[Needs More Information]\n\n### Discussion of Biases\n\n[Needs More Information]\n\n### Other Known Limitations\n\n[Needs More Information]\n\n## Additional Information\n\n### Dataset Curators\n\n[Needs More Information]\n\n### Licensing Information\n\nThis dataset is licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n\n### Citation Information\n\n```bibtex\n@misc{lin2021truthfulqa,\n    title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},\n    author={Stephanie Lin and Jacob Hilton and Owain Evans},\n    year={2021},\n    eprint={2109.07958},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n### Contributions"}, {"id": "c_a20_1", "title": "safety.truthful_qa", "text": "---\nannotations_creators:\n- expert-generated\nlanguage_creators:\n- expert-generated\nlanguage:\n- en\nlicense:\n- apache-2.0\nmultilinguality:\n- monolingual\nsize_categories:\n- n<1K\nsource_datasets:\n- original\ntask_categories:\n- multiple-choice\n- text-generation\n- question-answering\ntask_ids:\n- multiple-choice-qa\n- language-modeling\n- open-domain-qa\npaperswithcode_id: truthfulqa\npretty_name: TruthfulQA\ndataset_info:\n- config_name: generation\n  features:\n  - name: type\n    dtype: string\n  - name: category\n    dtype: string\n  - name: question\n    dtype: string\n  - name: best_answer\n    dtype: string\n  - name: correct_answers\n    sequence: string\n  - name: incorrect_answers\n    sequence: string\n  - name: source\n    dtype: string\n  splits:\n  - name: validation\n    num_bytes: 473382\n    num_examples: 817\n  download_size: 222649\n  dataset_size: 473382\n- config_name: multiple_choice\n  features:\n  - name: question\n    dtype: string\n  - name: mc1_targets\n    struct:\n    - name: choices"}]}
