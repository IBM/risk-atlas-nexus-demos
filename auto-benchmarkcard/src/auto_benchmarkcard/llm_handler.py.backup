"""Unified inference engine interface for multiple LLM backends.

This module provides a unified interface for working with multiple inference
backends including RITS, Ollama, vLLM, and WML, following the risk-atlas-nexus
architecture pattern.
"""

import json
import os
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Union

from dotenv import load_dotenv
from langchain_core.runnables import Runnable
from pydantic import BaseModel, TypeAdapter

# Load environment variables
load_dotenv()


class InferenceEngineType(str, Enum):
    """Supported inference engine types.

    Attributes:
        OLLAMA
        RITS: IBM Research Internal
        VLLM
        WML: Watson Machine Learning (IBM)
    """

    OLLAMA = "OLLAMA"
    RITS = "RITS"
    VLLM = "VLLM"
    WML = "WML"


class InferenceEngineCredentials(dict):
    """Credentials container for inference engines.

    Stores API credentials required for connecting to different inference
    backends. Inherits from dict for backward compatibility.

    Attributes:
        api_url: Base URL for the inference API.
        api_key: Authentication key (optional for local engines).
        space_id: Workspace identifier (engine-specific).
        project_id: Project identifier (engine-specific).
    """

    def __init__(
        self,
        api_url: str,
        api_key: Optional[str] = None,
        space_id: Optional[str] = None,
        project_id: Optional[str] = None,
    ):
        """Initialize credentials container.

        Args:
            api_url: Base URL for the inference API.
            api_key: Authentication key (optional for local engines).
            space_id: Workspace identifier (engine-specific).
            project_id: Project identifier (engine-specific).
        """
        super().__init__()
        self["api_url"] = api_url
        if api_key:
            self["api_key"] = api_key
        if space_id:
            self["space_id"] = space_id
        if project_id:
            self["project_id"] = project_id


@dataclass
class TextGenerationInferenceOutput:
    """Contains the prediction results and metadata for the inference.

    Attributes:
        prediction: Generated text or list of structured outputs.
        input_tokens: Number of tokens in the input.
        output_tokens: Number of tokens in the output.
        stop_reason: Reason the generation stopped.
        seed: Random seed used for generation.
        input_text: Original input text.
        model_name_or_path: Name or path of the model used.
        inference_engine: Type of inference engine used.
    """

    prediction: Union[str, List[Dict[str, Any]]]
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    stop_reason: Optional[str] = None
    seed: Optional[int] = None
    input_text: Optional[str] = None
    model_name_or_path: Optional[str] = None
    inference_engine: Optional[str] = None


class OllamaInferenceEngineParams(dict):
    """Parameters for Ollama inference engine.

    Supports Ollama-specific parameters including num_ctx (context window size),
    temperature (sampling temperature), top_k and top_p (sampling parameters),
    seed (random seed), and stop (stop sequences).
    """

    pass


class RITSInferenceEngineParams(dict):
    """Parameters for RITS inference engine.

    Supports OpenAI-compatible parameters including temperature (sampling
    temperature), max_completion_tokens (maximum tokens to generate), top_p
    (nucleus sampling parameter), frequency_penalty (penalty for frequent tokens),
    presence_penalty (penalty for repeated tokens), and seed (random seed).
    """

    pass


class VLLMInferenceEngineParams(dict):
    """Parameters for vLLM inference engine.

    Supports vLLM-specific parameters including temperature (sampling temperature),
    top_p and top_k (sampling parameters), max_tokens (maximum tokens to generate),
    presence_penalty and frequency_penalty (token penalties), repetition_penalty
    (repetition penalty), seed (random seed), and stop (stop sequences).
    """

    pass


class WMLInferenceEngineParams(dict):
    """Parameters for Watson Machine Learning (WML) inference engine.

    Supports WML-specific parameters including decoding_method ("greedy" or
    "sample"), temperature (sampling temperature), top_p and top_k (sampling
    parameters), max_new_tokens (maximum tokens to generate), min_new_tokens
    (minimum tokens to generate), repetition_penalty (repetition penalty),
    random_seed (random seed), and stop_sequences (stop sequences).
    """

    pass


class InferenceEngine(ABC):
    """Base class for inference engines (based on risk-atlas-nexus).

    Abstract base class that defines the interface for all inference engine
    implementations. Subclasses must implement prepare_credentials, create_client,
    generate, and chat methods.
    """

    def __init__(
        self,
        model_name_or_path: str,
        credentials: Optional[Union[Dict, InferenceEngineCredentials]] = None,
        parameters: Optional[Union[RITSInferenceEngineParams, OllamaInferenceEngineParams]] = None,
        concurrency_limit: int = 10,
    ):
        """Initialize the inference engine.

        Args:
            model_name_or_path: Model identifier or path.
            credentials: API credentials for the inference engine.
            parameters: Engine-specific generation parameters.
            concurrency_limit: Maximum number of concurrent requests.
        """
        self.model_name_or_path = model_name_or_path
        self.parameters = self._check_if_parameters_are_valid(parameters)
        self.client = self.create_client(
            self.prepare_credentials(credentials if credentials else {})
        )
        self.concurrency_limit = concurrency_limit

    def _check_if_parameters_are_valid(self, parameters):
        """Validate inference engine parameters.

        Args:
            parameters: Engine-specific parameters to validate.

        Returns:
            Validated parameters dictionary (empty dict if None provided).
        """
        if parameters:
            # For simplicity, accept any parameters for now
            pass
        return parameters or {}

    def _to_openai_format(self, prompt: Union[List[Dict[str, str]], str]):
        """Convert prompt to OpenAI message format.

        Args:
            prompt: Either a string or list of message dictionaries.

        Returns:
            List of message dictionaries with 'role' and 'content' keys.

        Raises:
            ValueError: If prompt format is invalid.
        """
        if isinstance(prompt, str):
            return [{"role": "user", "content": prompt}]
        elif isinstance(prompt, list):
            return prompt
        else:
            raise ValueError(f"Invalid input format: {prompt}")

    @abstractmethod
    def prepare_credentials(
        self, credentials: Union[Dict, InferenceEngineCredentials]
    ) -> InferenceEngineCredentials:
        """Prepare and validate credentials for the inference engine.

        Args:
            credentials: Raw credentials dict or InferenceEngineCredentials object.

        Returns:
            Validated InferenceEngineCredentials instance.

        Raises:
            ValueError: If required credentials are missing.
        """
        raise NotImplementedError

    @abstractmethod
    def create_client(self, credentials: InferenceEngineCredentials) -> Any:
        """Create the client for the inference engine.

        Args:
            credentials: Validated credentials for authentication.

        Returns:
            Initialized client instance for the specific engine.

        Raises:
            ImportError: If required client library not installed.
        """
        raise NotImplementedError

    @abstractmethod
    def generate(
        self,
        prompts: List[str],
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Generate text from prompts.

        Args:
            prompts: List of text prompts to generate from.
            response_format: Optional JSON schema for structured output.
            postprocessors: Optional postprocessing functions.
            verbose: Whether to print verbose output.

        Returns:
            List of TextGenerationInferenceOutput objects.
        """
        raise NotImplementedError

    @abstractmethod
    def chat(
        self,
        messages: Union[List[Dict[str, str]], List[str]],
        tools=None,
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Chat with the model.

        Args:
            messages: List of message dictionaries or strings.
            tools: Optional tool definitions for function calling.
            response_format: Optional JSON schema for structured output.
            postprocessors: Optional postprocessing functions.
            verbose: Whether to print verbose output.

        Returns:
            List of TextGenerationInferenceOutput objects.
        """
        raise NotImplementedError

    def generate_structured(self, prompt: str, response_schema: BaseModel) -> Any:
        """Generate structured output using Pydantic model.

        Args:
            prompt: Text prompt for generation.
            response_schema: Pydantic model defining the expected output structure.

        Returns:
            Validated instance of the response_schema.

        Raises:
            ValueError: If the generated output cannot be parsed or validated.
        """
        schema = response_schema.model_json_schema()
        result = self.generate([prompt], response_format=schema)

        try:
            # Parse the JSON response
            parsed = json.loads(result[0].prediction)
            # Validate with Pydantic
            return response_schema.model_validate(parsed)
        except (json.JSONDecodeError, ValueError) as e:
            # Fallback: try to extract JSON from text
            try:
                # Look for JSON in the response
                import re

                json_match = re.search(r"\{.*\}", result[0].prediction, re.DOTALL)
                if json_match:
                    parsed = json.loads(json_match.group())
                    return response_schema.model_validate(parsed)
            except:
                pass
            raise ValueError(f"Failed to parse structured output: {e}")


class OllamaInferenceEngine(InferenceEngine):
    """Ollama inference engine implementation."""

    _inference_engine_type = InferenceEngineType.OLLAMA
    _inference_engine_parameter_class = OllamaInferenceEngineParams

    def prepare_credentials(
        self, credentials: Union[Dict, InferenceEngineCredentials]
    ) -> InferenceEngineCredentials:
        api_url = credentials.get(
            "api_url",
            os.environ.get("OLLAMA_API_URL", "http://localhost:11434"),
        )
        return InferenceEngineCredentials(api_url=api_url)

    def create_client(self, credentials):
        try:
            from ollama import Client

            return Client(host=credentials["api_url"])
        except ImportError:
            raise ImportError("Please install ollama package: pip install ollama")

    def generate(
        self,
        prompts: List[str],
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Generate text from prompts using Ollama."""
        results = []

        for prompt in prompts:
            try:
                response = self.client.generate(
                    model=self.model_name_or_path,
                    prompt=prompt,
                    format=response_format,
                    options=self.parameters,
                )

                output = TextGenerationInferenceOutput(
                    prediction=response.response,
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                    input_text=prompt,
                )
                results.append(output)

            except Exception as e:
                if verbose:
                    print(f"Error generating with Ollama: {e}")
                raise

        return results

    def chat(
        self,
        messages: Union[List[Dict[str, str]], List[str]],
        tools=None,
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Chat with Ollama model."""
        results = []

        # Handle both single message and list of message sets
        if isinstance(messages, str) or (
            isinstance(messages, list) and len(messages) > 0 and isinstance(messages[0], str)
        ):
            # Single message or list of string messages
            message_sets = [messages] if isinstance(messages, str) else [[msg] for msg in messages]
        else:
            # List of message sets
            message_sets = [messages] if isinstance(messages[0], dict) else messages

        for message_set in message_sets:
            try:
                formatted_messages = self._to_openai_format(message_set)

                response = self.client.chat(
                    model=self.model_name_or_path,
                    messages=formatted_messages,
                    tools=tools,
                    format=response_format,
                    options=self.parameters,
                )

                output = TextGenerationInferenceOutput(
                    prediction=response.message.content,
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                    input_text=str(message_set),
                )
                results.append(output)

            except Exception as e:
                if verbose:
                    print(f"Error chatting with Ollama: {e}")
                raise

        return results


class RITSInferenceEngine(InferenceEngine):
    """RITS inference engine implementation."""

    _inference_engine_type = InferenceEngineType.RITS
    _inference_engine_parameter_class = RITSInferenceEngineParams

    def prepare_credentials(
        self, credentials: Union[Dict, InferenceEngineCredentials]
    ) -> InferenceEngineCredentials:
        api_key = credentials.get("api_key", os.environ.get("RITS_API_KEY", None))
        if not api_key:
            raise ValueError(
                f"Error while trying to run {self._inference_engine_type}. "
                f"Please set the env variable: 'RITS_API_KEY' or pass api_key to credentials."
            )

        api_url = credentials.get("api_url", os.environ.get("RITS_API_URL", None))
        if not api_url:
            raise ValueError(
                f"Error while trying to run {self._inference_engine_type}. "
                f"Please set the env variable: 'RITS_API_URL' or pass api_url to credentials."
            )

        return InferenceEngineCredentials(api_key=api_key, api_url=api_url)

    def create_client(self, credentials):
        try:
            from openai import OpenAI

            # Check if the API URL already includes the model path
            api_url = credentials["api_url"]
            if not api_url.endswith("/v1"):
                api_url = f"{api_url}/v1"

            return OpenAI(
                api_key=credentials["api_key"],
                base_url=api_url,
                default_headers={"RITS_API_KEY": credentials["api_key"]},
            )
        except ImportError:
            raise ImportError("Please install openai package: pip install openai")

    def generate(
        self,
        prompts: List[str],
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Generate text from prompts using RITS (delegates to chat)."""
        # Convert each prompt to a message format for chat
        message_sets = [[{"role": "user", "content": prompt}] for prompt in prompts]

        results = []
        for message_set in message_sets:
            try:
                # Prepare request parameters
                request_params = {
                    "messages": message_set,
                    "model": self.model_name_or_path,
                    **self.parameters,
                }

                # Add structured output format if provided
                if response_format:
                    request_params["response_format"] = self._create_schema_format(response_format)

                response = self.client.chat.completions.create(**request_params)

                output = TextGenerationInferenceOutput(
                    prediction=response.choices[0].message.content,
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                    input_text=message_set[0]["content"],
                )
                results.append(output)

            except Exception as e:
                if verbose:
                    print(f"Error generating with RITS: {e}")
                raise

        return results

    def chat(
        self,
        messages: Union[List[Dict[str, str]], List[str]],
        tools=None,
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Chat with RITS model."""
        results = []

        # Handle different input formats
        if isinstance(messages, str):
            # Single string message
            message_sets = [[messages]]
        elif isinstance(messages, list) and len(messages) > 0:
            if isinstance(messages[0], str):
                # List of string messages - each string becomes a separate chat
                message_sets = [[msg] for msg in messages]
            elif isinstance(messages[0], dict):
                # Already formatted OpenAI messages
                message_sets = [messages]
            else:
                # List of message sets
                message_sets = messages
        else:
            message_sets = [messages]

        for message_set in message_sets:
            try:
                formatted_messages = self._to_openai_format(message_set)

                # Prepare request parameters
                request_params = {
                    "messages": formatted_messages,
                    "model": self.model_name_or_path,
                    **self.parameters,
                }

                # Add structured output format if provided
                if response_format:
                    request_params["response_format"] = self._create_schema_format(response_format)

                response = self.client.chat.completions.create(**request_params)

                output = TextGenerationInferenceOutput(
                    prediction=response.choices[0].message.content,
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                    input_text=str(message_set),
                )
                results.append(output)

            except Exception as e:
                if verbose:
                    print(f"Error chatting with RITS: {e}")
                raise

        return results

    def _create_schema_format(self, response_format):
        """Create schema format for structured output.

        Args:
            response_format: JSON schema dict for response structure

        Returns:
            RITS-formatted schema dict or None
        """
        if response_format:
            return {
                "type": "json_schema",
                "json_schema": {
                    "name": "RITS_schema",
                    "schema": response_format,
                },
            }
        return None


class VLLMInferenceEngine(InferenceEngine):
    """vLLM inference engine implementation.

    Supports both offline (local vLLM instance) and online (vLLM server) modes.
    """

    _inference_engine_type = InferenceEngineType.VLLM
    _inference_engine_parameter_class = VLLMInferenceEngineParams

    def prepare_credentials(
        self, credentials: Union[Dict, InferenceEngineCredentials]
    ) -> InferenceEngineCredentials:
        """Prepare credentials for vLLM engine.

        If api_url is provided, runs in server mode. Otherwise runs offline.
        """
        if credentials:
            api_url = credentials.get(
                "api_url",
                os.environ.get("VLLM_API_URL", None),
            )
            if api_url:
                api_key = credentials.get("api_key", os.environ.get("VLLM_API_KEY", "-"))
                return InferenceEngineCredentials(api_url=api_url, api_key=api_key)

        # Offline mode - no credentials needed
        return None

    def create_client(self, credentials):
        """Create vLLM client (OpenAI for server mode, vLLM for offline)."""
        if credentials:
            # Server mode - use OpenAI client
            try:
                from openai import OpenAI

                return OpenAI(
                    api_key=credentials.get("api_key", "-"),
                    base_url=f"{credentials['api_url']}/v1",
                )
            except ImportError:
                raise ImportError("Please install openai package: pip install openai")
        else:
            # Offline mode - use vLLM directly
            try:
                from vllm import LLM

                return LLM(
                    model=self.model_name_or_path,
                    trust_remote_code=True,
                    max_model_len=4096,
                )
            except ImportError:
                raise ImportError("Please install vllm package: pip install vllm")

    def generate(
        self,
        prompts: List[str],
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Generate text from prompts using vLLM."""
        try:
            from vllm import LLM
        except ImportError:
            LLM = None

        if LLM and isinstance(self.client, LLM):
            # Offline mode
            from vllm import SamplingParams

            params = self.parameters.copy()
            if response_format:
                try:
                    from vllm.sampling_params import GuidedDecodingParams

                    params["guided_decoding"] = GuidedDecodingParams(json=response_format)
                except ImportError:
                    pass

            sampling_params = SamplingParams(**params)
            responses = self.client.generate(
                prompts=prompts, sampling_params=sampling_params, use_tqdm=verbose
            )

            results = []
            for response in responses:
                output = TextGenerationInferenceOutput(
                    prediction=response.outputs[0].text,
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                )
                results.append(output)
            return results
        else:
            # Server mode - use OpenAI-style chat completions
            results = []
            for prompt in prompts:
                try:
                    request_params = {
                        "messages": [{"role": "user", "content": prompt}],
                        "model": self.model_name_or_path,
                        **self.parameters,
                    }
                    if response_format:
                        request_params["response_format"] = self._create_schema_format(
                            response_format
                        )

                    response = self.client.chat.completions.create(**request_params)

                    output = TextGenerationInferenceOutput(
                        prediction=response.choices[0].message.content,
                        model_name_or_path=self.model_name_or_path,
                        inference_engine=str(self._inference_engine_type),
                        input_text=prompt,
                    )
                    results.append(output)
                except Exception as e:
                    if verbose:
                        print(f"Error generating with vLLM: {e}")
                    raise

            return results

    def chat(
        self,
        messages: Union[List[Dict[str, str]], List[str]],
        tools=None,
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Chat with vLLM model."""
        try:
            from vllm import LLM
        except ImportError:
            LLM = None

        if LLM and isinstance(self.client, LLM):
            # Offline mode
            from vllm import SamplingParams

            params = self.parameters.copy()
            if response_format:
                try:
                    from vllm.sampling_params import GuidedDecodingParams

                    params["guided_decoding"] = GuidedDecodingParams(json=response_format)
                except ImportError:
                    pass

            # Format messages
            if isinstance(messages, str):
                message_sets = [[messages]]
            elif isinstance(messages, list) and len(messages) > 0:
                if isinstance(messages[0], str):
                    message_sets = [[msg] for msg in messages]
                elif isinstance(messages[0], dict):
                    message_sets = [messages]
                else:
                    message_sets = messages
            else:
                message_sets = [messages]

            formatted_messages = [self._to_openai_format(msg) for msg in message_sets]

            sampling_params = SamplingParams(**params)
            responses = self.client.chat(
                messages=formatted_messages, sampling_params=sampling_params, use_tqdm=verbose
            )

            results = []
            for response in responses:
                output = TextGenerationInferenceOutput(
                    prediction=response.outputs[0].text,
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                )
                results.append(output)
            return results
        else:
            # Server mode
            results = []

            # Handle different input formats
            if isinstance(messages, str):
                message_sets = [[messages]]
            elif isinstance(messages, list) and len(messages) > 0:
                if isinstance(messages[0], str):
                    message_sets = [[msg] for msg in messages]
                elif isinstance(messages[0], dict):
                    message_sets = [messages]
                else:
                    message_sets = messages
            else:
                message_sets = [messages]

            for message_set in message_sets:
                try:
                    formatted_messages = self._to_openai_format(message_set)

                    request_params = {
                        "messages": formatted_messages,
                        "model": self.model_name_or_path,
                        **self.parameters,
                    }
                    if response_format:
                        request_params["response_format"] = self._create_schema_format(
                            response_format
                        )

                    response = self.client.chat.completions.create(**request_params)

                    output = TextGenerationInferenceOutput(
                        prediction=response.choices[0].message.content,
                        model_name_or_path=self.model_name_or_path,
                        inference_engine=str(self._inference_engine_type),
                        input_text=str(message_set),
                    )
                    results.append(output)
                except Exception as e:
                    if verbose:
                        print(f"Error chatting with vLLM: {e}")
                    raise

            return results

    def _create_schema_format(self, response_format):
        """Create schema format for structured output."""
        if response_format:
            return {
                "type": "json_schema",
                "json_schema": {
                    "name": "vllm_schema",
                    "schema": response_format,
                },
            }
        return None


class WMLInferenceEngine(InferenceEngine):
    """Watson Machine Learning (WML) inference engine implementation."""

    _inference_engine_type = InferenceEngineType.WML
    _inference_engine_parameter_class = WMLInferenceEngineParams

    def prepare_credentials(
        self, credentials: Union[Dict, InferenceEngineCredentials]
    ) -> InferenceEngineCredentials:
        """Prepare credentials for WML engine."""
        api_key = credentials.get("api_key", os.environ.get("WML_API_KEY", None))
        if not api_key:
            raise ValueError(
                "Error while trying to run WML. "
                "Please set the env variable: 'WML_API_KEY' or pass api_key to credentials."
            )

        api_url = credentials.get("api_url", os.environ.get("WML_API_URL", None))
        if not api_url:
            raise ValueError(
                "Error while trying to run WML. "
                "Please set the env variable: 'WML_API_URL' or pass api_url to credentials."
            )

        space_id = credentials.get("space_id", os.environ.get("WML_SPACE_ID", None))
        project_id = credentials.get("project_id", os.environ.get("WML_PROJECT_ID", None))

        if not (space_id or project_id):
            raise ValueError(
                "Error while trying to run WML. "
                "Either 'space_id' or 'project_id' must be specified. "
                "Please set WML_SPACE_ID or WML_PROJECT_ID environment variable "
                "or pass space_id/project_id to credentials."
            )

        return InferenceEngineCredentials(
            api_key=api_key, api_url=api_url, space_id=space_id, project_id=project_id
        )

    def create_client(self, credentials):
        """Create WML client."""
        try:
            from ibm_watsonx_ai import APIClient
            from ibm_watsonx_ai.foundation_models import ModelInference

            api_client = APIClient(
                {
                    "url": credentials["api_url"],
                    "apikey": credentials["api_key"],
                }
            )

            if credentials.get("space_id"):
                api_client.set.default_space(credentials["space_id"])
            else:
                api_client.set.default_project(credentials["project_id"])

            return ModelInference(
                model_id=self.model_name_or_path, api_client=api_client, params=self.parameters
            )
        except ImportError:
            raise ImportError(
                "Please install IBM Watson Machine Learning package: "
                "pip install ibm-watsonx-ai"
            )

    def generate(
        self,
        prompts: List[str],
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Generate text from prompts using WML."""
        responses = []

        try:
            for response in self.client.generate(
                prompt=prompts, params=self.parameters, concurrency_limit=self.concurrency_limit
            ):
                output = TextGenerationInferenceOutput(
                    prediction=response["results"][0]["generated_text"],
                    input_tokens=response["results"][0].get("input_token_count"),
                    output_tokens=response["results"][0].get("generated_token_count"),
                    stop_reason=response["results"][0].get("stop_reason"),
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                )
                responses.append(output)
        except Exception as e:
            if verbose:
                print(f"Error generating with WML: {e}")
            raise

        return responses

    def chat(
        self,
        messages: Union[List[Dict[str, str]], List[str]],
        tools=None,
        response_format=None,
        postprocessors=None,
        verbose=True,
    ) -> List[TextGenerationInferenceOutput]:
        """Chat with WML model."""
        results = []

        # Handle different input formats
        if isinstance(messages, str):
            message_sets = [[messages]]
        elif isinstance(messages, list) and len(messages) > 0:
            if isinstance(messages[0], str):
                message_sets = [[msg] for msg in messages]
            elif isinstance(messages[0], dict):
                message_sets = [messages]
            else:
                message_sets = messages
        else:
            message_sets = [messages]

        for message_set in message_sets:
            try:
                formatted_messages = self._to_openai_format(message_set)

                response = self.client.chat(messages=formatted_messages, params=self.parameters)

                output = TextGenerationInferenceOutput(
                    prediction=response["choices"][0]["message"]["content"],
                    input_tokens=response["usage"].get("prompt_tokens"),
                    output_tokens=response["usage"].get("completion_tokens"),
                    stop_reason=response["choices"][0].get("finish_reason"),
                    model_name_or_path=self.model_name_or_path,
                    inference_engine=str(self._inference_engine_type),
                    input_text=str(message_set),
                )
                results.append(output)
            except Exception as e:
                if verbose:
                    print(f"Error chatting with WML: {e}")
                raise

        return results


def create_inference_engine(
    engine_type: InferenceEngineType,
    model_name_or_path: str,
    credentials: Optional[Dict] = None,
    parameters: Optional[Dict] = None,
    concurrency_limit: int = 10,
) -> InferenceEngine:
    """Factory function to create inference engines."""

    if engine_type == InferenceEngineType.OLLAMA:
        return OllamaInferenceEngine(
            model_name_or_path=model_name_or_path,
            credentials=credentials,
            parameters=parameters,
            concurrency_limit=concurrency_limit,
        )
    elif engine_type == InferenceEngineType.RITS:
        return RITSInferenceEngine(
            model_name_or_path=model_name_or_path,
            credentials=credentials,
            parameters=parameters,
            concurrency_limit=concurrency_limit,
        )
    elif engine_type == InferenceEngineType.VLLM:
        return VLLMInferenceEngine(
            model_name_or_path=model_name_or_path,
            credentials=credentials,
            parameters=parameters,
            concurrency_limit=concurrency_limit,
        )
    elif engine_type == InferenceEngineType.WML:
        return WMLInferenceEngine(
            model_name_or_path=model_name_or_path,
            credentials=credentials,
            parameters=parameters,
            concurrency_limit=concurrency_limit,
        )
    else:
        raise ValueError(f"Unsupported engine type: {engine_type}")


# Default model configurations
DEFAULT_OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3:27b")
DEFAULT_RITS_MODEL = os.getenv("RITS_MODEL", "meta-llama/Llama-3.1-80B-Instruct")
DEFAULT_VLLM_MODEL = os.getenv("VLLM_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
DEFAULT_WML_MODEL = os.getenv("WML_MODEL", "ibm/granite-13b-chat-v2")


def create_ollama_engine(
    model_name: Optional[str] = None,
    credentials: Optional[Dict] = None,
    parameters: Optional[Dict] = None,
) -> OllamaInferenceEngine:
    """Create Ollama inference engine with default settings.

    Args:
        model_name: Model name/path (defaults to DEFAULT_OLLAMA_MODEL)
        credentials: Credentials dict with api_url
        parameters: OllamaInferenceEngineParams

    Returns:
        Configured OllamaInferenceEngine instance
    """
    return create_inference_engine(
        InferenceEngineType.OLLAMA,
        model_name or DEFAULT_OLLAMA_MODEL,
        credentials,
        parameters,
    )


def create_rits_engine(
    model_name: Optional[str] = None,
    credentials: Optional[Dict] = None,
    parameters: Optional[Dict] = None,
) -> RITSInferenceEngine:
    """Create RITS inference engine with default settings.

    Args:
        model_name: Model name/path (defaults to DEFAULT_RITS_MODEL)
        credentials: Credentials dict with api_key and api_url
        parameters: RITSInferenceEngineParams

    Returns:
        Configured RITSInferenceEngine instance
    """
    return create_inference_engine(
        InferenceEngineType.RITS,
        model_name or DEFAULT_RITS_MODEL,
        credentials,
        parameters,
    )


def create_vllm_engine(
    model_name: Optional[str] = None,
    credentials: Optional[Dict] = None,
    parameters: Optional[Dict] = None,
) -> VLLMInferenceEngine:
    """Create vLLM inference engine with default settings.

    Args:
        model_name: Model name/path (defaults to DEFAULT_VLLM_MODEL)
        credentials: Credentials dict with api_url (None for offline mode)
        parameters: VLLMInferenceEngineParams

    Returns:
        Configured VLLMInferenceEngine instance
    """
    return create_inference_engine(
        InferenceEngineType.VLLM,
        model_name or DEFAULT_VLLM_MODEL,
        credentials,
        parameters,
    )


def create_wml_engine(
    model_name: Optional[str] = None,
    credentials: Optional[Dict] = None,
    parameters: Optional[Dict] = None,
) -> WMLInferenceEngine:
    """Create WML inference engine with default settings.

    Args:
        model_name: Model name/path (defaults to DEFAULT_WML_MODEL)
        credentials: Credentials dict with api_key, api_url, space_id/project_id
        parameters: WMLInferenceEngineParams

    Returns:
        Configured WMLInferenceEngine instance
    """
    return create_inference_engine(
        InferenceEngineType.WML,
        model_name or DEFAULT_WML_MODEL,
        credentials,
        parameters,
    )


class LLMHandler:
    """Main LLM handler that manages different inference engines.

    Unified interface for working with multiple inference backends (RITS, Ollama, vLLM, WML).

    Args:
        engine_type: Type of inference engine ("ollama", "rits", "vllm", "wml")
        model_name: Model identifier (uses defaults if not specified)
        **kwargs: Additional arguments passed to engine constructor (credentials, parameters)

    Example:
        >>> # RITS engine
        >>> handler = LLMHandler(engine_type="rits", model_name="meta-llama/Llama-3.1-80B-Instruct")
        >>> response = handler.generate("What is machine learning?")

        >>> # vLLM offline mode
        >>> handler = LLMHandler(engine_type="vllm", model_name="meta-llama/Llama-3.1-8B-Instruct")
        >>> response = handler.generate("Explain transformers")

        >>> # WML with credentials
        >>> handler = LLMHandler(
        ...     engine_type="wml",
        ...     credentials={"api_key": "...", "api_url": "...", "space_id": "..."}
        ... )
    """

    def __init__(self, engine_type: str = "ollama", model_name: Optional[str] = None, **kwargs):
        self.engine_type = engine_type.upper()

        if self.engine_type == "OLLAMA":
            self.model_name = model_name or DEFAULT_OLLAMA_MODEL
            self.engine = create_ollama_engine(self.model_name, **kwargs)
        elif self.engine_type == "RITS":
            self.model_name = model_name or DEFAULT_RITS_MODEL
            self.engine = create_rits_engine(self.model_name, **kwargs)
        elif self.engine_type == "VLLM":
            self.model_name = model_name or DEFAULT_VLLM_MODEL
            self.engine = create_vllm_engine(self.model_name, **kwargs)
        elif self.engine_type == "WML":
            self.model_name = model_name or DEFAULT_WML_MODEL
            self.engine = create_wml_engine(self.model_name, **kwargs)
        else:
            raise ValueError(
                f"Unsupported engine type: {engine_type}. "
                f"Supported types: OLLAMA, RITS, VLLM, WML"
            )

    def generate(self, prompt: str, response_format: Optional[Dict] = None) -> str:
        """Generate text response."""
        result = self.engine.generate([prompt], response_format)
        return result[0].prediction

    def generate_structured(self, prompt: str, response_schema: BaseModel) -> Any:
        """Generate structured output using Pydantic model."""
        return self.engine.generate_structured(prompt, response_schema)

    def chat(
        self,
        messages: Union[List[Dict[str, str]], str],
        response_format: Optional[Dict] = None,
    ) -> str:
        """Chat with the model."""
        result = self.engine.chat(messages, response_format=response_format)
        return result[0].prediction

    def with_structured_output(self, schema: BaseModel):
        """Create a version of the handler that outputs structured data."""

        class StructuredHandler(Runnable):
            def __init__(self, handler, schema):
                self.handler = handler
                self.schema = schema

            def invoke(self, input_data, config=None) -> Any:
                # Handle different input formats
                if isinstance(input_data, dict):
                    if "text" in input_data:
                        prompt = input_data["text"]
                    elif "query" in input_data:
                        prompt = input_data["query"]
                    elif "messages" in input_data:
                        # Handle LangChain message format
                        messages = input_data["messages"]
                        if isinstance(messages, list) and len(messages) > 0:
                            if isinstance(messages[-1], dict) and "content" in messages[-1]:
                                prompt = messages[-1]["content"]
                            else:
                                prompt = str(messages[-1])
                        else:
                            prompt = str(messages)
                    else:
                        prompt = str(input_data)
                else:
                    prompt = str(input_data)

                return self.handler.generate_structured(prompt, self.schema)

        return StructuredHandler(self, schema)


# Create default LLM instance for backward compatibility
def get_llm_handler(engine_type: str = "ollama", **kwargs) -> LLMHandler:
    """Get an LLM handler instance."""
    return LLMHandler(engine_type=engine_type, **kwargs)


# For backward compatibility with existing code
LLM = get_llm_handler()
